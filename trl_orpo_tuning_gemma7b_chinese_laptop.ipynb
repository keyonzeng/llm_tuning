{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tuning Gemma 7b with ORPO and QLora on your laptop to enhance Chinese capability\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial can run on your laptop with NVIDIA GPU. \n",
    "you should install CUDA 12.3，Pycharm/VSode and PyTorch 2.2.1 beforehand.\n",
    "\n",
    "Dataset: wenbopan/Chinese-dpo-pairs\n",
    "\n",
    "## Setup\n",
    "### download gemma-7b model from huggingface\n",
    "[https://huggingface.co/google/gemma-7b/tree/main](https://huggingface.co/google/gemma-7b/tree/main)\n",
    "Note: I don't like the cache model mechanism of huggingface, \n",
    "\n",
    "### Configure your wandb key\n",
    "\n",
    "To use wandb to monitor, you must provide wandb API key. you can apply API key from [https://wandb.ai](https://wandb.ai)\n",
    "\n",
    "### Configure your Hugging Face access token\n",
    "\n",
    "if you want to upload your tuned LLM to Hugging Face, you can apply access token (write) from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "\n",
    "### Set environment variables\n",
    "\n",
    "Set environment variables for `wandb` and `huggingface`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31f016f9edf2a3b"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "(True, '12.1')"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make sure that CUDA works\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available(), torch.version.cuda"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T16:25:30.855687500Z",
     "start_time": "2024-03-29T16:25:28.570691600Z"
    }
   },
   "id": "921860dc631bb307"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "{'prompt': '任务定义：您将获得一个亚马逊食品产品的评论以及其极性（积极或消极）。您的任务是回答“True”，如果指定的句子及其极性匹配；否则，回答“False”。\\n问题：我买了这些很长时间，因为我认为它们是美国原产的，所以对这个产品感觉更好。我购买了几袋（条状、包装的苹果等），现在意识到原产地是中国。考虑到所有关于包装的FDA法规，是时候要求原产地在包装的正面显著显示（比如20号字体 - 而不是微不足道的写法，没有人会注意到！）！因此，我将不再购买Dogswell产品。另外，生皮更糟糕，因为狗可以摄入更高剂量的任何用于处理皮革的毒素（而且你可以肯定有很多！）。我甚至拿起一包名为（类似）“U.S.A.生皮”的产品，背面微小的字体写着“中国制造”。为什么我们的立法者不采取一些有益的行动，打击这种行为？这最多是狡猾，最坏的情况下是故意欺骗！亚马逊，请为我们的“毛孩子”找到不会让它们生病或致命的产品！\\n极性：积极\\n\\n解决方案：False\\n\\n问题：您想从花生酱中去除糖而不是脂肪。老实说，与之相比，这个味道很糟糕。花生中大约50％的脂肪是单不饱和脂肪。不饱和脂肪可以帮助降低血液中的LDL胆固醇（“坏”胆固醇）水平，而不影响HDL胆固醇（“好”胆固醇）。研究表明，食用更高比例的单不饱和脂肪的人降低了患心脏病、哮喘、阿尔茨海默病、乳腺癌甚至抑郁症的风险。只需购买天然种类，避免添加剂。\\n极性：积极\\n\\n解决方案：False\\n\\n问题：我在意识到我得到了多少之前就买了这个，也在知道杂货店有这个产品之前！那是我的错。这些种子很棒，只是我冰箱里有太多了！\\n极性：消极\\n\\n解决方案：',\n 'system': '',\n 'chosen': '错误\\n\\n问题：我订购了这些牛肉棒作为肉干的健康替代品，但很失望地发现它们含有添加糖。我尽量避免摄入添加糖，所以我不能吃这些。在亚马逊上的配料中没有列出这一点，所以我建议在购买前检查包装。\\n\\n极性：负面\\n\\n解决方案：',\n 'rejected': '真的',\n 'source': 'flan_v2_niv2',\n 'id': None}"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load wenbopan/Chinese-dpo-pairsdataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "#dataset = load_dataset(\"argilla/distilabel-capybara-dpo-7k-binarized\",split=\"train\")\n",
    "#dataset2 = load_dataset(\"allenai/ultrafeedback_binarized_cleaned\",split=\"train\")\n",
    "\n",
    "dataset = load_dataset(\"wenbopan/Chinese-dpo-pairs\",split=\"train\")\n",
    "\n",
    "\n",
    "dataset[0]\n",
    "\n",
    "#dataset.to_csv(\"a.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T16:15:40.043781800Z",
     "start_time": "2024-03-29T16:15:22.093917Z"
    }
   },
   "id": "8e3d969e9935d26f"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "{'prompt': '<bos><start_of_turn>user\\n任务定义：给出一段泰米尔语的文本。将其从泰米尔语翻译成英语。翻译不能省略或添加原句的信息。\\n问题：கேரளாவின் வெள்ள நிலைமைகளை ஆய்வு செய்ய இரண்டு நாட்களில் இரண்டாவது முறையாக தேசிய நெருக்கடி மேலாண்மைக் குழு கூடியது\\n\\n解决方案：国家内政部 NCMC 在两天内第二次会议上，审查喀拉拉邦的洪水情况\\n\\n问题：வெகு விரைவில் உங்களது வீட்டின் சாவிகள் உங்களுக்கு கிடைக்கும் என்று நான் உறுதி கூறுகிறேன்” என்றும் பிரதமர் தெரிவித்தார்.\\n\\n解决方案：我向您保证，很快您将拥有您家的钥匙。\\n\\n问题：இஸ்ரோவின் தொழில்நுட்ப தொடர்பு கூடத்தில் பணிபுரியவுள்ள தொடர்பு அலுவலர், ரஷ்யாவிலும், அண்டை நாடுகளிலும், இஸ்ரோவின் செயல்திட்ட இலக்குகள் தொடர்பான பணிகளை ஒருங்கிணைப்பார்.\\n\\n解决方案：以色列的技术合作伙伴，包括俄罗斯、印度等国家，将与以色列的行动计划相关的活动进行协调。<end_of_turn>\\n<start_of_turn>model\\n',\n 'chosen': '当然，我很乐意帮助你！给定的泰米尔语句子“கேரளாவின் வெள்ள நிலைமைகளை ஆய்வு செய்ய இரண்டு நாட்களில் இரண்டாவது முறையாக தேசிய நெருக்கடி மேலாண்மைக் குழு கூடியது” 可以翻译成英文为“The Ministry of Home Affairs NCMC meets for the second time in two days to review the flood situation in Kerala.”\\n句子“வெகு விரைவில் உங்களது வீட்டின் சாवிකள் உங்களுக்கு கிடைக்கும் என்று நான் உறுதி கூறுகிறேன்” 可以翻译成英文为“I assure you that very soon you will have the keys of your home in your hands.”\\n句子“இஸ்ரோவின் தொழில்நுட்ப தொடர்பு கூடத்தில் பணிபுரிவுள்ள தொடர்பு அலுவலர், ரஷ்யாவிலும், அண்டை நாடுகளிலும், இஸ்ரோவின் செயல்திட்ட இலக்குகள் தொடர்பான பணிகளை ஒருங்கிணைப்பார்” 可以翻译成英文为“[The Cabinet Secretary] has assured that the necessary actions will be taken to implement the decisions taken by the NCMC, which includes providing additional funds for flood relief, conducting an immediate review of the flood situation, and ensuring the timely distribution of relief materials to the affected areas.”\\n希望这能帮到你！如果有其他问题，请告诉我。<eos>',\n 'rejected': '捐助者们在耶塞尔·阿拉法特的问题导向计划中停滞了两年，讨论了以色列境内阿拉伯解放机构和失望的巴勒斯坦难民营的问题。<eos>'}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#format dataset format\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def chatml_format(example):\n",
    "    message = {\"role\": \"user\", \"content\": example['system']+example['prompt']}\n",
    "    # Format instruction\n",
    "    prompt = tokenizer.apply_chat_template([message], tokenize=False, add_generation_prompt=True)\n",
    "    # Format chosen answer\n",
    "    chosen = example['chosen']+tokenizer.eos_token\n",
    "    # Format rejected answer\n",
    "    rejected = example['rejected']+tokenizer.eos_token\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "    }\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"wenbopan/Chinese-dpo-pairs\",split=\"train\")\n",
    "#dataset = load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\")\n",
    "#dataset = dataset.filter(\n",
    "#   lambda r: \n",
    "#       r[\"status\"] != \"tie\" and \n",
    "#       r[\"chosen_score\"] >= 5\n",
    "#       and not r[\"in_gsm8k_train\"]\n",
    "#)\n",
    "# Save columns\n",
    "original_columns = dataset.column_names\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "model_name = \"c:/ai/models/gemma\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "\n",
    "# Format dataset\n",
    "dataset = dataset.map(\n",
    "    chatml_format,\n",
    "    remove_columns=original_columns\n",
    ")\n",
    "\n",
    "# Print sample\n",
    "dataset[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T16:26:55.341383900Z",
     "start_time": "2024-03-29T16:26:38.528499200Z"
    }
   },
   "id": "49010f1deb279cb2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "781b6ebacc62afd8"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mkevon-zeng\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\kevon\\.netrc\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08c67d7bb02a4148a228838a286a9514"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888888925108, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "340bde181a8042e9b37459b487811f20"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.2"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\codes\\FineTuning_CUDA12\\wandb\\run-20240330_002824-5k30zdut</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/kevon-zeng/huggingface/runs/5k30zdut' target=\"_blank\">expert-sun-43</a></strong> to <a href='https://wandb.ai/kevon-zeng/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/kevon-zeng/huggingface' target=\"_blank\">https://wandb.ai/kevon-zeng/huggingface</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/kevon-zeng/huggingface/runs/5k30zdut' target=\"_blank\">https://wandb.ai/kevon-zeng/huggingface/runs/5k30zdut</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/670 : < :, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=670, training_loss=2.0626583168755714, metrics={'train_runtime': 15616.5652, 'train_samples_per_second': 0.687, 'train_steps_per_second': 0.043, 'total_flos': 0.0, 'train_loss': 2.0626583168755714, 'epoch': 1.0})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using ORPOTrainer and QLora to tune Gemma 7B to enhance Chinese capability\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel\n",
    "import wandb\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from trl import ORPOTrainer\n",
    "from trl import ORPOConfig\n",
    "\n",
    "#init env\n",
    "env =load_dotenv(find_dotenv())\n",
    "hf_token = os.getenv(\"huggingface\")\n",
    "wb_token = os.getenv('wandb')\n",
    "wandb.login(key=wb_token)\n",
    "\n",
    "#local model path\n",
    "local_model_path =\"c:/ai/models/gemma\"\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
    ")\n",
    "\n",
    "# Model to fine-tune\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage =True,\n",
    "    #torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    quantization_config= BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "    )\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "new_model = \"lion-gemma-7b-cn\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Training arguments\n",
    "training_args = ORPOConfig (\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant':True},\n",
    "    remove_unused_columns=False,\n",
    "    learning_rate=8e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    #max_steps=400,\n",
    "    num_train_epochs=1,\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=1,\n",
    "    output_dir=new_model,\n",
    "    optim=\"adamw_bnb_8bit\",\n",
    "    warmup_steps=40,\n",
    "    bf16=True,\n",
    "    max_prompt_length=256,\n",
    "    max_length=1024,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "# Create DPO trainer\n",
    "orpo_trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config\n",
    ")\n",
    "\n",
    "# Fine-tune model with ORPO\n",
    "orpo_trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T20:48:41.030727500Z",
     "start_time": "2024-03-29T16:27:44.870333800Z"
    }
   },
   "id": "9ebb5c7e7d72fab"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\anaconda3\\envs\\FineTuning_CUDA12\\Lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in c:/ai/models/gemma - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "('gemma_final_checkpoint\\\\tokenizer_config.json',\n 'gemma_final_checkpoint\\\\special_tokens_map.json',\n 'gemma_final_checkpoint\\\\tokenizer.model',\n 'gemma_final_checkpoint\\\\added_tokens.json',\n 'gemma_final_checkpoint\\\\tokenizer.json')"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save tuning checkpoint\n",
    "\n",
    "final_checkpoint = \"gemma_final_checkpoint\"\n",
    "orpo_trainer.model.save_pretrained(final_checkpoint)\n",
    "tokenizer.save_pretrained(final_checkpoint)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T02:09:10.128306500Z",
     "start_time": "2024-03-30T02:09:09.572891600Z"
    }
   },
   "id": "3dd307b090fb5357"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0bcb1bfceebc43f3865daa4647b71397"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "('lion-gemma-7b-cn\\\\tokenizer_config.json',\n 'lion-gemma-7b-cn\\\\special_tokens_map.json',\n 'lion-gemma-7b-cn\\\\tokenizer.model',\n 'lion-gemma-7b-cn\\\\added_tokens.json',\n 'lion-gemma-7b-cn\\\\tokenizer.json')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge checkpoint with original llm\n",
    "env =load_dotenv(find_dotenv(),override=True)\n",
    "hf_token = os.getenv(\"huggingface\")\n",
    "\n",
    "#Flush memory\n",
    "del orpo_trainer, model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reload model in FP16 (instead of NF4)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "\n",
    "# Merge base model with the adapter\n",
    "model = PeftModel.from_pretrained(base_model, final_checkpoint)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T02:11:28.540403500Z",
     "start_time": "2024-03-30T02:09:51.359835800Z"
    }
   },
   "id": "6fba55e2a84f3298"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d80037ffa8a4fcfa08ea1a33202da08"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e1e273a18364e51b50bdac2029e72d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08b3f18b7c544e8b8706d02c6b4499cc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9de25420038540988949dcdb848a82e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bebffd6d3099478b9ceeefce4b7b6471"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27a79ed7652e4ea89ec71c3f6ab700ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\anaconda3\\envs\\FineTuning_CUDA12\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kevon\\.cache\\huggingface\\hub\\models--KeyonZeng--lion-gemma-7b-cn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00dfaa62798d42a1aa6218903d4909d8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e054361afc345e2b3d3365bd4281a31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef26daf4bab64ccda14c259ca5bc7ecb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "CommitInfo(commit_url='https://huggingface.co/KeyonZeng/lion-gemma-7b-cn/commit/ed194b039d098d4e0eaaddcf2d99bdaad52de5bc', commit_message='Upload tokenizer', commit_description='', oid='ed194b039d098d4e0eaaddcf2d99bdaad52de5bc', pr_url=None, pr_revision=None, pr_num=None)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push them to the HF Hub\n",
    "os.environ[\"HTTPS_PROXY\"] =\"http://127.0.0.1:7890\"\n",
    "model.push_to_hub(new_model, use_temp_dir=False, token=hf_token)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False, token=hf_token)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T03:17:50.767449200Z",
     "start_time": "2024-03-30T02:11:43.809146200Z"
    }
   },
   "id": "7867ef58ef889fe8"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6863a300146b474aa32eb1a7233ce868"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "请向小学生解释一下什么是大语言模型<end_of_turn>\n",
      "<start_of_turn>model\n",
      "大语言模型是一种特殊的计算机程序，可以像我们一样说话，理解和生成文本。它们可以学习很多东西，比如我们人类可以学习的知识，比如语言、数学和科学。\n",
      "\n",
      "大语言模型可以帮助我们做很多事情，比如回答问题、提供信息、帮助我们学习新的东西，甚至创作故事。它们可以帮助我们理解世界，并帮助我们做出更好的决定。\n",
      "\n",
      "大语言模型可以学习很多东西，比如我们人类可以学习的知识，比如语言、数学和科学。它们可以帮助我们做很多事情，比如回答问题、提供信息、帮助我们学习新的东西，甚至\n"
     ]
    }
   ],
   "source": [
    "#test tuned gemma-7b model\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "new_model = \"lion-gemma-7b-cn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(new_model,torch_dtype=torch.bfloat16, device_map=\"cuda\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    new_model,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\", \n",
    "    quantization_config= BitsAndBytesConfig\n",
    "        (\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "    ))  # You may want to use bfloat16 and/or move to GPU here\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"请向小学生解释一下什么是大语言模型\"},\n",
    " ]\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "\n",
    "tokenized_chat =tokenized_chat.to('cuda')\n",
    "outputs = model.generate(tokenized_chat, max_new_tokens=128, temperature = 0.1)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T04:38:47.549311Z",
     "start_time": "2024-03-30T04:37:42.905845800Z"
    }
   },
   "id": "7e2479f07a9658d7"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0525776eec74ed09f9b3bd6d8f46204"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "请向小学生解释一下什么是大语言模型<end_of_turn>\n",
      "<start_of_turn>model\n",
      "大语言模型 (LLM) 是一个特殊的计算机程序，可以像一个人一样理解和生成大量的文本信息。\n",
      "\n",
      "**大语言模型的主要特点:**\n",
      "\n",
      "* **学习大量的文本数据:** LLM 在训练过程中，会学习大量的文本数据，包括书籍、文章、代码和网页上的内容。\n",
      "* **生成文本:** LLM 可以根据输入信息生成新的文本，例如故事、文章、代码或翻译。\n",
      "* **理解语言:** LLM 可以理解语言结构和语法，以及文本的意思。\n",
      "* **对话:** LLM 可以与人进行对话，并提供信息、帮助或娱乐。\n"
     ]
    }
   ],
   "source": [
    "# compare generated content by gemma-7b base model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "old_model = \"c:/ai/models/gemma\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(new_model,torch_dtype=torch.bfloat16, device_map=\"cuda\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    old_model,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\", \n",
    "    quantization_config= BitsAndBytesConfig\n",
    "        (\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "    ))  # You may want to use bfloat16 and/or move to GPU here\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"请向小学生解释一下什么是大语言模型\"},\n",
    " ]\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "\n",
    "tokenized_chat =tokenized_chat.to('cuda')\n",
    "outputs = model.generate(tokenized_chat, max_new_tokens=128, temperature = 0.1 )\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T04:44:03.704341500Z",
     "start_time": "2024-03-30T04:42:45.047438900Z"
    }
   },
   "id": "1a5cda3c0c594409"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "507a40a5655e6d9b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
