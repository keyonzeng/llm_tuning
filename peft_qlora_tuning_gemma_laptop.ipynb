{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "32e7669cd82042cbbb419e25db606c1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b6698be32bf74c4087e129fab6e13fdd",
       "IPY_MODEL_ff7333b35c1c472482df6550f6e43be2",
       "IPY_MODEL_da4df56a1ba440dbb69087d0019cab1d"
      ],
      "layout": "IPY_MODEL_ad598693c58549e0a83a1328d77b8f83"
     }
    },
    "b6698be32bf74c4087e129fab6e13fdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de2f7a60851f4681877a4c8dccba29cc",
      "placeholder": "​",
      "style": "IPY_MODEL_02b296efbff143f4bfbb904cbc7b1109",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "ff7333b35c1c472482df6550f6e43be2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72ac83e43e2b4d4498070a5b701a5572",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_320fa615d4de4652ac34fc2518f7749e",
      "value": 3
     }
    },
    "da4df56a1ba440dbb69087d0019cab1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75280ef205a245be92da268e0752dc71",
      "placeholder": "​",
      "style": "IPY_MODEL_3f33eabd6f7f46ef8138abe748d8fbb1",
      "value": " 3/3 [01:06&lt;00:00, 18.14s/it]"
     }
    },
    "ad598693c58549e0a83a1328d77b8f83": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de2f7a60851f4681877a4c8dccba29cc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "02b296efbff143f4bfbb904cbc7b1109": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "72ac83e43e2b4d4498070a5b701a5572": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "320fa615d4de4652ac34fc2518f7749e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "75280ef205a245be92da268e0752dc71": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f33eabd6f7f46ef8138abe748d8fbb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fine Tuning Gemma with PEFT and QLora on your laptop \n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial can run on  of your laptop with NVIDIA GPU. \n",
    "you should install CUDA 12.3，Pycharm/VSode and PyTorch 2.1.2 beforehand.\n",
    "\n",
    "Dataset: argilla/databricks-dolly-15k-curated-en\n",
    "\n",
    "## Setup\n",
    "### download gemma-2b model from huggingface\n",
    "[https://huggingface.co/google/gemma-2b/tree/main](https://huggingface.co/google/gemma-2b/tree/main)\n",
    "Note: I don't like the cache model mechanism of huggingface, \n",
    "\n",
    "### Configure your wandb key\n",
    "\n",
    "To use wandb to monitor, you must provide wandb API key. you can apply API key from [https://wandb.ai](https://wandb.ai)\n",
    "\n",
    "### Set environment variables\n",
    "\n",
    "Set environment variables for `wandb`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#install the required dependencies\n",
    "!pip3 install -q -U python-dotenv\n",
    "!pip3 install -q -U https://github.com/jakaline-dev/bitsandbytes_win/releases/download/0.42.0/bitsandbytes-0.42.0-cp311-cp311-win_amd64.whl\n",
    "!pip3 install -q -U peft==0.8.2\n",
    "!pip3 install -q -U trl==0.7.10\n",
    "!pip3 install -q -U accelerate==0.27.1\n",
    "!pip3 install -q -U datasets==2.17.0\n",
    "!pip3 install -q -U transformers==4.39.0.dev0"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5gJk3W_s0RY",
    "outputId": "ca3d427e-5bfc-4635-f27a-e49e56718f7e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting git+https://****@github.com/huggingface/new-model-addition-golden-gate@add-golden-gate\n",
      "  Cloning https://****@github.com/huggingface/new-model-addition-golden-gate (to revision add-golden-gate) to /tmp/pip-req-build-8jci0sy8\n",
      "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/huggingface/new-model-addition-golden-gate' /tmp/pip-req-build-8jci0sy8\n",
      "  Running command git checkout -b add-golden-gate --track origin/add-golden-gate\n",
      "  Switched to a new branch 'add-golden-gate'\n",
      "  Branch 'add-golden-gate' set up to track remote branch 'add-golden-gate' from 'origin'.\n",
      "  Resolved https://****@github.com/huggingface/new-model-addition-golden-gate to commit e9d36beb5fcafeb2ac327a68eee82009d24cb58f\n",
      "  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0.dev0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0.dev0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0.dev0) (2024.2.2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GemmaTokenizer\n",
    "\n",
    "#I don't like huggingface cache model mechanism, so i download gemma model to local\n",
    "# use base model from local path\n",
    "#if you want to use huggingface model directly, uncomment the following code\n",
    "#base_model_path=\"google/gemma-2b\n",
    "base_model_path = \"c:/ai/models/gemma\"\n",
    "\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = GemmaTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "#using low_cpu_mem_usage since model is quantized\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_path,quantization_config=bnb_config,low_cpu_mem_usage=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "32e7669cd82042cbbb419e25db606c1d",
      "b6698be32bf74c4087e129fab6e13fdd",
      "ff7333b35c1c472482df6550f6e43be2",
      "da4df56a1ba440dbb69087d0019cab1d",
      "ad598693c58549e0a83a1328d77b8f83",
      "de2f7a60851f4681877a4c8dccba29cc",
      "02b296efbff143f4bfbb904cbc7b1109",
      "72ac83e43e2b4d4498070a5b701a5572",
      "320fa615d4de4652ac34fc2518f7749e",
      "75280ef205a245be92da268e0752dc71",
      "3f33eabd6f7f46ef8138abe748d8fbb1"
     ]
    },
    "id": "EVEotZX8s-v6",
    "outputId": "e378234f-f56f-483e-c569-f3a196c02370",
    "ExecuteTime": {
     "end_time": "2024-02-29T02:34:02.883725600Z",
     "start_time": "2024-02-29T02:33:47.210526600Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "710fc3de9acf487083c5240ff0343056"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# just to test the base model response\n",
    "\n",
    "text = \"Instruction: Can you explain contrastive learning in machine learning in simple terms for someone new to the field of ML?\\n Response:\"\n",
    "device = \"cuda:0\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=512)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Msk610TVUGW",
    "outputId": "8c14afe0-dc6e-42b1-d05a-1a7a6c2ace9e",
    "ExecuteTime": {
     "end_time": "2024-02-29T02:34:45.926520300Z",
     "start_time": "2024-02-29T02:34:30.107164700Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### USER: Can you explain contrastive learning in machine learning in simple terms for someone new to the field of ML?### Assistant: Sure, here's a simple explanation of contrastive learning in machine learning:\n",
      "\n",
      "**Contrastive learning** is a type of learning algorithm that focuses on learning relationships between data points rather than learning the features of individual points. It's like learning the similarities and differences between different items.\n",
      "\n",
      "Here's an analogy: Imagine you have two similar pictures of the same object. You want to teach a machine to recognize that they are similar. Instead of focusing on the specific features of each picture, contrastive learning would learn the similarities between the pictures as a whole.\n",
      "\n",
      "**Here are the key concepts of contrastive learning:**\n",
      "\n",
      "* **Positive and negative pairs:** The algorithm learns from pairs of data points, where one pair is similar and the other pair is dissimilar.\n",
      "* **Contrastive loss:** This loss function measures the similarity between positive and negative pairs.\n",
      "* **Metric learning:** The algorithm learns a distance metric that measures the similarity between data points.\n",
      "\n",
      "**Contrastive learning has been successful in various tasks:**\n",
      "\n",
      "* **Image recognition:** Learning similarity between images.\n",
      "* **Natural language processing:** Learning similarity between sentences.\n",
      "* **Object detection:** Identifying similar objects in images.\n",
      "\n",
      "**Overall, contrastive learning is a powerful technique for learning relationships between data points. It's particularly useful for tasks where traditional feature-based learning methods are not effective.**\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#Configure Lora\n",
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 8\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias= \"none\",\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ],
   "metadata": {
    "id": "7lzjoG3KVRMN",
    "ExecuteTime": {
     "end_time": "2024-02-29T02:54:56.535683500Z",
     "start_time": "2024-02-29T02:54:56.473799300Z"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Load argilla/databricks-dolly-15k-curated-en dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"argilla/databricks-dolly-15k-curated-en\")\n",
    "\n",
    "dataset"
   ],
   "metadata": {
    "id": "HPQSpLNAuubn",
    "ExecuteTime": {
     "end_time": "2024-02-29T04:08:43.760050400Z",
     "start_time": "2024-02-29T04:08:35.147628Z"
    }
   },
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'category', 'original-instruction', 'original-context', 'original-response', 'new-instruction', 'new-context', 'new-response', 'external_id'],\n        num_rows: 15015\n    })\n})"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "#just try small dataset \n",
    "from datasets import Dataset\n",
    "data = []\n",
    "for d in dataset['train']:\n",
    "    # Filter out examples with context, to keep it simple.\n",
    "    if d[\"original-context\"]:\n",
    "       continue\n",
    "    # Format the entire example as a single string.\n",
    "    #print(\"original-context:\"+d['original-context'])\n",
    "    template = \"Instruction:\\n{instruction}\\n{context}\\n\\nResponse:\\n{response}\".format(instruction=d['new-instruction']['value'],context=d['new-context']['value'],response=d['new-response']['value'])\n",
    "    #print(d['new-instruction']['value'])\n",
    "    #print(template)\n",
    "    #break\n",
    "    data.append({\"text\":template})\n",
    "\n",
    "# Only use 1000 training examples, to keep it fast.\n",
    "train_dataset = Dataset.from_list(data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T06:07:14.253437200Z",
     "start_time": "2024-02-29T06:07:12.918245600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text'],\n    num_rows: 10417\n})"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T06:07:21.362236300Z",
     "start_time": "2024-02-29T06:07:21.350889100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import transformers\n",
    "import os\n",
    "import  wandb\n",
    "from dotenv import find_dotenv,load_dotenv\n",
    "from trl import SFTTrainer\n",
    "\n",
    "env =load_dotenv(find_dotenv())\n",
    "\n",
    "wandb.login(key=os.environ['wandb'])\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=2000,\n",
    "        learning_rate=5e-5,\n",
    "        fp16=True,\n",
    "        max_grad_norm=0.3,\n",
    "        gradient_checkpointing=True,\n",
    "        group_by_length=True,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_steps=10,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"wandb\"\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    #formatting_func=formatting_func,\n",
    ")\n",
    "trainer.train()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "HFbR2FIgVfiT",
    "outputId": "ba27fbda-54be-415c-ee47-78632e4ad4c6",
    "ExecuteTime": {
     "end_time": "2024-02-29T07:37:11.644988100Z",
     "start_time": "2024-02-29T06:07:47.101825500Z"
    }
   },
   "execution_count": 92,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\kevon\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/10417 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57e00fc7f23b420ea1b71cbb0bb3034a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\anaconda3\\envs\\FineTuning_CUDA12\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:284: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011277777777286247, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c05a050450ab474aa30c28084c0c0b15"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.2"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\codes\\FineTuning_CUDA12\\wandb\\run-20240229_140753-6g4rf7o0</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/kevon-zeng/huggingface/runs/6g4rf7o0' target=\"_blank\">exalted-totem-23</a></strong> to <a href='https://wandb.ai/kevon-zeng/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/kevon-zeng/huggingface' target=\"_blank\">https://wandb.ai/kevon-zeng/huggingface</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/kevon-zeng/huggingface/runs/6g4rf7o0' target=\"_blank\">https://wandb.ai/kevon-zeng/huggingface/runs/6g4rf7o0</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "C:\\dev\\anaconda3\\envs\\FineTuning_CUDA12\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/2000 : < :, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\anaconda3\\envs\\FineTuning_CUDA12\\Lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in c:/ai/models/gemma - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\dev\\anaconda3\\envs\\FineTuning_CUDA12\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\dev\\anaconda3\\envs\\FineTuning_CUDA12\\Lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in c:/ai/models/gemma - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\dev\\anaconda3\\envs\\FineTuning_CUDA12\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\dev\\anaconda3\\envs\\FineTuning_CUDA12\\Lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in c:/ai/models/gemma - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "C:\\dev\\anaconda3\\envs\\FineTuning_CUDA12\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\dev\\anaconda3\\envs\\FineTuning_CUDA12\\Lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in c:/ai/models/gemma - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=2000, training_loss=1.7383782362937927, metrics={'train_runtime': 5357.9932, 'train_samples_per_second': 1.493, 'train_steps_per_second': 0.373, 'total_flos': 4.18592474370048e+16, 'train_loss': 1.7383782362937927, 'epoch': 0.77})"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "text = \"Instruction: Can you explain contrastive learning in machine learning in simple terms for someone new to the field of ML?\\n\\n Response:\"\n",
    "device = \"cuda:0\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=512)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5Mim0lNViwe",
    "outputId": "4534ee26-63e3-4ced-ee27-673f0b9d7afb",
    "ExecuteTime": {
     "end_time": "2024-02-29T11:02:55.964788800Z",
     "start_time": "2024-02-29T11:02:49.321814800Z"
    }
   },
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Can you explain contrastive learning in machine learning in simple terms for someone new to the field of ML?\n",
      "\n",
      " Response: Contrastive learning is a type of machine learning that is used to learn from unlabeled data. It is a type of learning that is used to learn from unlabeled data. The idea is to learn from the similarities and differences between pairs of data points.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.model.save_pretrained(\"gemma-a\")\n",
    "tokenizer.save_pretrained(\"gemma-a\")"
   ],
   "metadata": {
    "id": "djg3QAMuVx8R",
    "ExecuteTime": {
     "end_time": "2024-02-29T11:03:56.229719200Z",
     "start_time": "2024-02-29T11:03:55.941260700Z"
    }
   },
   "execution_count": 95,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\anaconda3\\envs\\FineTuning_CUDA12\\Lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in c:/ai/models/gemma - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "('gemma-a\\\\tokenizer_config.json',\n 'gemma-a\\\\special_tokens_map.json',\n 'gemma-a\\\\tokenizer.model',\n 'gemma-a\\\\added_tokens.json')"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85453d5b5b5d43c195de6ca9f44cd9d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "('lion-gemma\\\\tokenizer_config.json',\n 'lion-gemma\\\\special_tokens_map.json',\n 'lion-gemma\\\\tokenizer.model',\n 'lion-gemma\\\\added_tokens.json',\n 'lion-gemma\\\\tokenizer.json')"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "base_model_path =\"c:/ai/models/gemma\"\n",
    "new_model =\"lion-gemma\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "# Merge base model with the adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"gemma-a\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T11:07:43.667375100Z",
     "start_time": "2024-02-29T11:04:12.542391800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9cd2ef7261d4ccabee6252c46276136"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Can you explain contrastive learning in machine learning in simple terms for someone new to the field of ML?\n",
      "\n",
      " Response: In simple terms, contrastive learning is a type of machine learning that is used to learn the relationships between data points. It is a type of learning that is used to learn the relationships between data points. The goal of contrastive learning is to learn the relationships between data points so that the model can be used to make predictions about new data points.\n"
     ]
    }
   ],
   "source": [
    "n_tokenizer = AutoTokenizer.from_pretrained(new_model,device_map = \"cuda\")\n",
    "n_model = AutoModelForCausalLM.from_pretrained(new_model,quantization_config=bnb_config,device_map=\"cuda\")\n",
    "text = \"Instruction: Can you explain contrastive learning in machine learning in simple terms for someone new to the field of ML?\\n\\n Response:\"\n",
    "device = \"cuda\"\n",
    "inputs = n_tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = n_model.generate(**inputs, max_new_tokens=512)\n",
    "print(n_tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T11:11:37.036285900Z",
     "start_time": "2024-02-29T11:09:43.266374400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
