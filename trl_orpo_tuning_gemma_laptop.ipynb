{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tuning Gemma 2b with ORPO and QLora on your laptop \n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial can run on your laptop with NVIDIA GPU. \n",
    "you should install CUDA 12.3ï¼ŒPycharm/VSode and PyTorch 2.2.1 beforehand.\n",
    "\n",
    "Dataset: argilla/databricks-dolly-15k-curated-en\n",
    "\n",
    "## Setup\n",
    "### download gemma-2b model from huggingface\n",
    "[https://huggingface.co/google/gemma-2b/tree/main](https://huggingface.co/google/gemma-2b/tree/main)\n",
    "Note: I don't like the cache model mechanism of huggingface, \n",
    "\n",
    "### Configure your wandb key\n",
    "\n",
    "To use wandb to monitor, you must provide wandb API key. you can apply API key from [https://wandb.ai](https://wandb.ai)\n",
    "\n",
    "### Configure your Hugging Face access token\n",
    "\n",
    "if you want to upload your tuned LLM to Hugging Face, you can apply access token (write) from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "\n",
    "### Set environment variables\n",
    "\n",
    "Set environment variables for `wandb` and `huggingface`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31f016f9edf2a3b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#make sure that CUDA works\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available(), torch.version.cuda"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "921860dc631bb307"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load argilla/dpo-mix-7k dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "#dataset = load_dataset(\"argilla/distilabel-capybara-dpo-7k-binarized\",split=\"train\")\n",
    "#dataset2 = load_dataset(\"allenai/ultrafeedback_binarized_cleaned\",split=\"train\")\n",
    "\n",
    "dataset = load_dataset(\"argilla/dpo-mix-7k\",split=\"train\")\n",
    "\n",
    "\n",
    "dataset[0][\"chosen\"][0][\"content\"]\n",
    "\n",
    "#dataset.to_csv(\"a.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8e3d969e9935d26f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#format dataset format\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def chatml_format(example):\n",
    "    message = {\"role\": \"user\", \"content\": example['chosen'][0]['content']}\n",
    "    # Format instruction\n",
    "    prompt = tokenizer.apply_chat_template([message], tokenize=False, add_generation_prompt=True)\n",
    "    # Format chosen answer\n",
    "    chosen = example['chosen'][1]['content']+tokenizer.eos_token\n",
    "    # Format rejected answer\n",
    "    rejected = example['rejected'][1]['content']+tokenizer.eos_token\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "    }\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"argilla/dpo-mix-7k\",split=\"train\")\n",
    "#dataset = load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\")\n",
    "#dataset = dataset.filter(\n",
    "#   lambda r: \n",
    "#       r[\"status\"] != \"tie\" and \n",
    "#       r[\"chosen_score\"] >= 5\n",
    "#       and not r[\"in_gsm8k_train\"]\n",
    "#)\n",
    "# Save columns\n",
    "original_columns = dataset.column_names\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "model_name = \"c:/ai/models/gemma\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "\n",
    "# Format dataset\n",
    "dataset = dataset.map(\n",
    "    chatml_format,\n",
    "    remove_columns=original_columns\n",
    ")\n",
    "\n",
    "# Print sample\n",
    "dataset[1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49010f1deb279cb2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "781b6ebacc62afd8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Using ORPOTrainer and QLora to tune Gemma 2B\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel\n",
    "import wandb\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from trl import ORPOTrainer\n",
    "from trl import ORPOConfig\n",
    "\n",
    "#init env\n",
    "env =load_dotenv(find_dotenv())\n",
    "hf_token = os.getenv(\"huggingface\")\n",
    "wb_token = os.getenv('wandb')\n",
    "wandb.login(key=wb_token)\n",
    "\n",
    "#local model path\n",
    "local_model_path =\"c:/ai/models/gemma\"\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
    ")\n",
    "\n",
    "# Model to fine-tune\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    #torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config= BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "    )\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "new_model = \"lion-gemma-2b\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Training arguments\n",
    "training_args = ORPOConfig (\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant':True},\n",
    "    remove_unused_columns=False,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    #max_steps=400,\n",
    "    num_train_epochs=1,\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=1,\n",
    "    output_dir=new_model,\n",
    "    optim=\"adamw_bnb_8bit\",\n",
    "    warmup_steps=80,\n",
    "    bf16=True,\n",
    "    max_prompt_length=256,\n",
    "    max_length=1024,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "# Create DPO trainer\n",
    "orpo_trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config\n",
    ")\n",
    "\n",
    "# Fine-tune model with ORPO\n",
    "orpo_trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ebb5c7e7d72fab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save tuning checkpoint\n",
    "\n",
    "final_checkpoint = \"gemma_final_checkpoint\"\n",
    "orpo_trainer.model.save_pretrained(final_checkpoint)\n",
    "tokenizer.save_pretrained(final_checkpoint)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dd307b090fb5357"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# merge checkpoint with original llm\n",
    "env =load_dotenv(find_dotenv(),override=True)\n",
    "hf_token = os.getenv(\"huggingface\")\n",
    "\n",
    "#Flush memory\n",
    "del orpo_trainer, model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reload model in FP16 (instead of NF4)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "\n",
    "# Merge base model with the adapter\n",
    "model = PeftModel.from_pretrained(base_model, final_checkpoint)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fba55e2a84f3298"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b9aaf3e35984a8f8f4aaa9502da75f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e5ca7c3b50e4e5a89198076d2c14c83"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32e3ef52b00b49c48b14de07e5c04286"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Push them to the HF Hub\n",
    "os.environ[\"HTTPS_PROXY\"] =\"http://127.0.0.1:7890\"\n",
    "model.push_to_hub(new_model, use_temp_dir=False, token=hf_token)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False, token=hf_token)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-26T13:03:20.286297800Z"
    }
   },
   "id": "7867ef58ef889fe8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test the new llm\n",
    "message = [\n",
    "    {\"role\": \"user\", \"content\": \"What is a Large Language Model?\"}\n",
    "]\n",
    "tokenizer = AutoTokenizer.from_pretrained(new_model)\n",
    "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=new_model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=200,\n",
    ")\n",
    "print(sequences[0]['generated_text'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cefa60fc6569dd23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env =load_dotenv(find_dotenv(),override=True)\n",
    "\n",
    "hf_token = os.getenv(\"huggingface\")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(new_model)\n",
    "tokenizer =AutoTokenizer.from_pretrained(new_model)\n",
    "model.push_to_hub(model,token = hf_token)\n",
    "tokenizer.push_to_hub(model, token =hf_token)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e380a59620596fd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
